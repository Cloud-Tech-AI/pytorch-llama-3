{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import collections\n",
    "import regex\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiktoken BPE Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Class for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonBPE:\n",
    "    @staticmethod\n",
    "    def encode(\n",
    "        mergeable_ranks: dict[bytes, int], input: bytes, visualise: str | None = \"colour\"\n",
    "    ) -> list[int]:\n",
    "        parts = [bytes([b]) for b in input]\n",
    "        while True:\n",
    "            # See the intermediate merges play out!\n",
    "            if visualise:\n",
    "                if visualise in [\"colour\", \"color\"]:\n",
    "                    PythonBPE.visualise_tokens(parts)\n",
    "                elif visualise == \"simple\":\n",
    "                    print(parts)\n",
    "\n",
    "            # Iterate over all pairs and find the pair we want to merge the most\n",
    "            min_idx = None\n",
    "            min_rank = None\n",
    "            for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "                rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "                if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                    min_idx = i\n",
    "                    min_rank = rank\n",
    "\n",
    "            # If there were no pairs we could merge, we're done!\n",
    "            if min_rank is None:\n",
    "                break\n",
    "            assert min_idx is not None\n",
    "\n",
    "            # Otherwise, merge that pair and leave the rest unchanged. Then repeat.\n",
    "            parts = (\n",
    "                parts[:min_idx]\n",
    "                + [parts[min_idx] + parts[min_idx + 1]]\n",
    "                + parts[min_idx + 2 :]\n",
    "            )\n",
    "\n",
    "        if visualise:\n",
    "            print()\n",
    "\n",
    "        tokens = [mergeable_ranks[part] for part in parts]\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def train(\n",
    "        data: str, vocab_size: int, pat_str: str, visualise: str | None = \"colour\"\n",
    "    ) -> dict[bytes, int]:\n",
    "        # First, add tokens for each individual byte value\n",
    "        if vocab_size < 2**8:\n",
    "            raise ValueError(\"vocab_size must be at least 256, so we can encode all bytes\")\n",
    "        ranks = {}\n",
    "        for i in range(2**8):\n",
    "            ranks[bytes([i])] = i\n",
    "\n",
    "        # Splinter up our data into lists of bytes\n",
    "        # data = \"Hello world\"\n",
    "        # words = [\n",
    "        #     [b'H', b'e', b'l', b'l', b'o'],\n",
    "        #     [b' ', b'w', b'o', b'r', b'l', b'd']\n",
    "        # ]\n",
    "        words: list[list[bytes]] = [\n",
    "            [bytes([b]) for b in word.encode(\"utf-8\")]\n",
    "            for word in regex.findall(pat_str, data)\n",
    "        ]\n",
    "\n",
    "        # Now, use our data to figure out which merges we should make\n",
    "        while len(ranks) < vocab_size:\n",
    "            # Find the most common pair. This will become our next token\n",
    "            stats = collections.Counter()\n",
    "            for piece in words:\n",
    "                for pair in zip(piece[:-1], piece[1:]):\n",
    "                    stats[pair] += 1\n",
    "            \n",
    "            most_common_pair = max(stats, key=lambda x: stats[x])\n",
    "            token_bytes = most_common_pair[0] + most_common_pair[1]\n",
    "            token = len(ranks)\n",
    "            # Add the new token!\n",
    "            ranks[token_bytes] = token\n",
    "\n",
    "            # Now merge that most common pair in all the words. That is, update our training data\n",
    "            # to reflect our decision to make that pair into a new token.\n",
    "            new_words = []\n",
    "            for word in words:\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word) - 1:\n",
    "                    if (word[i], word[i + 1]) == most_common_pair:\n",
    "                        # We found our pair! Merge it\n",
    "                        new_word.append(token_bytes)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word[i])\n",
    "                        i += 1\n",
    "                if i == len(word) - 1:\n",
    "                    new_word.append(word[i])\n",
    "                new_words.append(new_word)\n",
    "            words = new_words\n",
    "\n",
    "            # See the intermediate merges play out!\n",
    "            if visualise:\n",
    "                print(\n",
    "                    f\"The current most common pair is {most_common_pair[0]} + {most_common_pair[1]}\"\n",
    "                )\n",
    "                print(f\"So we made {token_bytes} our {len(ranks)}th token\")\n",
    "                if visualise in [\"colour\", \"color\"]:\n",
    "                    print(\"Now the first fifty words in our training data look like:\")\n",
    "                    PythonBPE.visualise_tokens([token for word in words[:50] for token in word])\n",
    "                elif visualise == \"simple\":\n",
    "                    print(\"Now the first twenty words in our training data look like:\")\n",
    "                    for word in words[:20]:\n",
    "                        print(word)\n",
    "                print(\"\\n\")\n",
    "\n",
    "        return ranks\n",
    "\n",
    "    @staticmethod\n",
    "    def visualise_tokens(token_values: list[bytes]) -> None:\n",
    "        background = [f\"\\u001b[48;5;{i}m\" for i in [167, 179, 185, 77, 80, 68, 134]]\n",
    "        # If token boundaries do not occur at unicode character boundaries, it's unclear how best to\n",
    "        # visualise the token. Here, we'll just use the unicode replacement character to represent some\n",
    "        # fraction of a character.\n",
    "        unicode_token_values = [x.decode(\"utf-8\", errors=\"replace\") for x in token_values]\n",
    "\n",
    "        running_length = 0\n",
    "        last_color = None\n",
    "        for token in unicode_token_values:\n",
    "            color = background[running_length % len(background)]\n",
    "            if color == last_color:\n",
    "                color = background[(running_length + 1) % len(background)]\n",
    "                assert color != last_color\n",
    "            last_color = color\n",
    "            running_length += len(token)\n",
    "            print(color + token, end=\"\")\n",
    "        print(\"\\u001b[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runner Class for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBytePairEncoding:\n",
    "    def __init__(self, *, pat_str: str, mergeable_ranks: dict[bytes, int]) -> None:\n",
    "        \"\"\"Creates an Encoding object.\"\"\"\n",
    "        # A regex pattern string that is used to split the input text\n",
    "        self.pat_str = pat_str\n",
    "        # A dictionary mapping token bytes to their ranks. The ranks correspond to merge priority\n",
    "        self.mergeable_ranks = mergeable_ranks\n",
    "\n",
    "        self._decoder = {\n",
    "            token: token_bytes for token_bytes, token in mergeable_ranks.items()\n",
    "        }\n",
    "        self._pat = regex.compile(pat_str)\n",
    "\n",
    "    def encode(self, text: str, visualise: str | None = \"colour\") -> list[int]:\n",
    "        \"\"\"Encodes a string into tokens.\n",
    "\n",
    "        >>> enc.encode(\"hello world\")\n",
    "        [388, 372]\n",
    "        \"\"\"\n",
    "        # Use the regex to split the text into (approximately) words\n",
    "        words = self._pat.findall(text)\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            # Turn each word into tokens, using the byte pair encoding algorithm\n",
    "            word_bytes = word.encode(\"utf-8\")\n",
    "            word_tokens = PythonBPE.encode(\n",
    "                self.mergeable_ranks, word_bytes, visualise=visualise\n",
    "            )\n",
    "            tokens.extend(word_tokens)\n",
    "        return tokens\n",
    "\n",
    "    def decode_bytes(self, tokens: list[int]) -> bytes:\n",
    "        \"\"\"Decodes a list of tokens into bytes.\n",
    "\n",
    "        >>> enc.decode_bytes([388, 372])\n",
    "        b'hello world'\n",
    "        \"\"\"\n",
    "        return b\"\".join(self._decoder[token] for token in tokens)\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Decodes a list of tokens into a string.\n",
    "\n",
    "        Decoded bytes are not guaranteed to be valid UTF-8. In that case, we replace\n",
    "        the invalid bytes with the replacement character \"ï¿½\".\n",
    "\n",
    "        >>> enc.decode([388, 372])\n",
    "        'hello world'\n",
    "        \"\"\"\n",
    "        return self.decode_bytes(tokens).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def decode_tokens_bytes(self, tokens: list[int]) -> list[bytes]:\n",
    "        \"\"\"Decodes a list of tokens into a list of bytes.\n",
    "\n",
    "        Useful for visualising how a string is tokenised.\n",
    "\n",
    "        >>> enc.decode_tokens_bytes([388, 372])\n",
    "        [b'hello', b' world']\n",
    "        \"\"\"\n",
    "        return [self._decoder[token] for token in tokens]\n",
    "\n",
    "    @staticmethod\n",
    "    def train(training_data: str, vocab_size: int, pat_str: str):\n",
    "        \"\"\"Train a BPE tokeniser on some data!\"\"\"\n",
    "        mergeable_ranks = PythonBPE.train(\n",
    "            data=training_data, vocab_size=vocab_size, pat_str=pat_str\n",
    "        )\n",
    "        return SimpleBytePairEncoding(pat_str=pat_str, mergeable_ranks=mergeable_ranks)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_tiktoken(encoding):\n",
    "        if isinstance(encoding, str):\n",
    "            encoding = tiktoken.get_encoding(encoding)\n",
    "        return SimpleBytePairEncoding(\n",
    "            pat_str=encoding._pat_str, mergeable_ranks=encoding._mergeable_ranks\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_pattern = r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?[\\p{L}]+| ?[\\p{N}]+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "file_path = input(\"Enter the file path: \") or '<your-file-path>' # e.g. 'data.txt'\n",
    "\n",
    "with open(file_path) as f:\n",
    "    data = f.read()\n",
    "\n",
    "bpe_cls = SimpleBytePairEncoding.train(data, vocab_size=260, pat_str=gpt2_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"This is the sequence of merges performed in order to encode 'hello world':\")\n",
    "tokens = bpe_cls.encode(\"hello world\")\n",
    "assert bpe_cls.decode(tokens) == \"hello world\"\n",
    "assert bpe_cls.decode_bytes(tokens) == b\"hello world\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
